{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b4102b",
   "metadata": {},
   "source": [
    "# For Free Rider Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9de7e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base code\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, temperature, lr, endowment, MPCR, incentive):\n",
    "        actions = np.linspace(0, endowment, num=num_actions)\n",
    "        self.Q = np.array([[np.random.normal((endowment - action + (MPCR * (1+incentive)) * action), (endowment - action + (MPCR) * action) * temperature/100) for action in actions] for _ in range(num_states)])\n",
    "        self.temperature = temperature  \n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.state_counts = np.full(num_states, temperature)\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def update_state_counts(self, observed_state):\n",
    "        self.state_counts[observed_state] += 1\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def predict_state(self):\n",
    "        predicted_state = np.random.choice(self.num_states, p=self.state_frequencies)\n",
    "        return predicted_state\n",
    "    \n",
    "    def softmax(self, q_values):\n",
    "        if self.temperature > 0:\n",
    "            max_q = np.max(q_values)\n",
    "            exp_q = np.exp((q_values - max_q) / self.temperature)\n",
    "            probabilities = exp_q / np.sum(exp_q)\n",
    "        else:\n",
    "            probabilities = np.zeros_like(q_values)\n",
    "            probabilities[np.argmax(q_values)] = 1.0\n",
    "        return probabilities\n",
    "\n",
    "    def choose_action(self, use_most_likely_state=False):\n",
    "        if use_most_likely_state:\n",
    "            predicted_state = np.argmax(self.state_frequencies)\n",
    "        else:\n",
    "            predicted_state = self.predict_state()\n",
    "        q_values = self.Q[predicted_state]\n",
    "        probabilities = self.softmax(q_values)\n",
    "        action = np.random.choice(len(q_values), p=probabilities)\n",
    "        return action, predicted_state\n",
    "    \n",
    "    def update_Q(self, state, action, reward):\n",
    "        self.Q[state][action] = self.lr * reward + (1 - self.lr) * self.Q[state][action]\n",
    "\n",
    "\n",
    "def discretize_total_contribution(total_contribution, num_states, N, E):\n",
    "    max_total = N * E\n",
    "    state = int(round((total_contribution / max_total) * (num_states - 1)))\n",
    "    state = min(state, num_states - 1)\n",
    "    return state\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, N, E, MR, num_rounds, temperature=10, lr=0.5, num_states=11, num_actions=11, incentive=0):\n",
    "        self.N = N                  \n",
    "        self.E = E                 \n",
    "        self.MR = MR            \n",
    "        self.num_rounds = num_rounds\n",
    "        self.initial_temperature = temperature\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.incentive = incentive \n",
    "        self.agents = [Agent(num_states=self.num_states, num_actions=self.num_actions, temperature=self.initial_temperature, lr=self.lr, endowment=self.E, MPCR=self.MR/self.N, incentive=self.incentive) for _ in range(N)]\n",
    "        self.action_values = np.linspace(0, self.E, num=self.num_actions)\n",
    "    \n",
    "    def run_simulation(self):\n",
    "        total_contributions_over_time = []\n",
    "        prev_total_contribution = 0\n",
    "        prev_state = discretize_total_contribution(prev_total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "        for round_num in range(self.num_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action()\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * (1+self.incentive) * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions] # proportional incentive\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        final_contributions = []\n",
    "        final_rounds = 1\n",
    "        for agent in self.agents:\n",
    "            agent.temperature = 0 \n",
    "\n",
    "        for round_num in range(final_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action(use_most_likely_state=True)\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "            final_contributions.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * (1+self.incentive) * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions] # proportional incentive\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        average_final_contribution = np.mean(final_contributions)\n",
    "        self.result = average_final_contribution\n",
    "        self.record = total_contributions_over_time\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1ff2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "E = 100 \n",
    "MR = 5\n",
    "num_rounds = 1000\n",
    "temperature = 10\n",
    "lr = 1\n",
    "\n",
    "incentive = 0.5\n",
    "\n",
    "env = Environment(N=N, E=E, MR=MR, num_rounds=num_rounds, temperature=temperature, lr=lr, incentive=incentive)\n",
    "env.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341d832",
   "metadata": {},
   "source": [
    "# For Transparency Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "233c3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, temperature, lr, endowment, MPCR, transparency):\n",
    "        actions = np.linspace(0, endowment, num=num_actions)\n",
    "        self.Q = np.array([[np.random.normal((endowment - action + (MPCR) * action), (endowment - action + (MPCR) * action) * temperature/100) for action in actions] for _ in range(num_states)])\n",
    "        self.temperature = temperature  \n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.state_counts = np.full(num_states, temperature)\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def update_state_counts(self, observed_state):\n",
    "        self.state_counts[observed_state] += 1\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def predict_state(self):\n",
    "        predicted_state = np.random.choice(self.num_states, p=self.state_frequencies)\n",
    "        return predicted_state\n",
    "    \n",
    "    def softmax(self, q_values):\n",
    "        if self.temperature > 0:\n",
    "            max_q = np.max(q_values)\n",
    "            exp_q = np.exp((q_values - max_q) / self.temperature)\n",
    "            probabilities = exp_q / np.sum(exp_q)\n",
    "        else:\n",
    "            probabilities = np.zeros_like(q_values)\n",
    "            probabilities[np.argmax(q_values)] = 1.0\n",
    "        return probabilities\n",
    "\n",
    "    def choose_action(self, use_most_likely_state=False):\n",
    "        if use_most_likely_state:\n",
    "            predicted_state = np.argmax(self.state_frequencies)\n",
    "        else:\n",
    "            predicted_state = self.predict_state()\n",
    "        q_values = self.Q[predicted_state]\n",
    "        probabilities = self.softmax(q_values)\n",
    "        action = np.random.choice(len(q_values), p=probabilities)\n",
    "        return action, predicted_state\n",
    "    \n",
    "    def update_Q(self, state, action, reward):\n",
    "        self.Q[state][action] = self.lr * reward + (1 - self.lr) * self.Q[state][action]\n",
    "\n",
    "\n",
    "def discretize_total_contribution(total_contribution, num_states, N, E):\n",
    "    max_total = N * E\n",
    "    state = int(round((total_contribution / max_total) * (num_states - 1)))\n",
    "    state = min(state, num_states - 1)\n",
    "    return state\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, N, E, MR, num_rounds, temperature=10, lr=0.5, num_states=11, num_actions=11, transparency=1):\n",
    "        self.N = N                  \n",
    "        self.E = E                 \n",
    "        self.MR = MR            \n",
    "        self.num_rounds = num_rounds\n",
    "        self.initial_temperature = temperature\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.transparency = transparency\n",
    "        self.agents = [Agent(num_states=self.num_states, num_actions=self.num_actions, temperature=self.initial_temperature, lr=self.lr, endowment=self.E, MPCR=self.MR/self.N, transparency=self.transparency) for _ in range(N)]\n",
    "        self.action_values = np.linspace(0, self.E, num=self.num_actions)\n",
    "    \n",
    "    def run_simulation(self):\n",
    "        total_contributions_over_time = []\n",
    "        prev_total_contribution = 0\n",
    "        prev_state = discretize_total_contribution(prev_total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "        for round_num in range(self.num_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action()\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * self.transparency * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions]\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        final_contributions = []\n",
    "        final_rounds = 1\n",
    "        for agent in self.agents:\n",
    "            agent.temperature = 0 \n",
    "\n",
    "        for round_num in range(final_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action(use_most_likely_state=True)\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "            final_contributions.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * self.transparency * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions]\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        average_final_contribution = np.mean(final_contributions)\n",
    "        self.result = average_final_contribution\n",
    "        self.record = total_contributions_over_time\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d531cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10          \n",
    "E = 100 \n",
    "MR = 10\n",
    "num_rounds = 1000\n",
    "temperature = 10\n",
    "lr = 1\n",
    "\n",
    "transparency = 0.6\n",
    "\n",
    "env = Environment(N=N, E=E, MR=MR, num_rounds=num_rounds, temperature=temperature, lr=lr, transparency=transparency)\n",
    "env.run_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
