{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773dfe02",
   "metadata": {},
   "source": [
    "# For Free Rider Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58695e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base code\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, temperature, lr, endowment, MPCR, incentive):\n",
    "        actions = np.linspace(0, endowment, num=num_actions)\n",
    "        self.Q = np.array([[np.random.normal((endowment - action + (MPCR * (1+incentive)) * action), (endowment - action + (MPCR) * action) * temperature/100) for action in actions] for _ in range(num_states)])\n",
    "        self.temperature = temperature  \n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.state_counts = np.full(num_states, temperature)\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def update_state_counts(self, observed_state):\n",
    "        self.state_counts[observed_state] += 1\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def predict_state(self):\n",
    "        predicted_state = np.random.choice(self.num_states, p=self.state_frequencies)\n",
    "        return predicted_state\n",
    "    \n",
    "    def softmax(self, q_values):\n",
    "        if self.temperature > 0:\n",
    "            max_q = np.max(q_values)\n",
    "            exp_q = np.exp((q_values - max_q) / self.temperature)\n",
    "            probabilities = exp_q / np.sum(exp_q)\n",
    "        else:\n",
    "            probabilities = np.zeros_like(q_values)\n",
    "            probabilities[np.argmax(q_values)] = 1.0\n",
    "        return probabilities\n",
    "\n",
    "    def choose_action(self, use_most_likely_state=False):\n",
    "        if use_most_likely_state:\n",
    "            predicted_state = np.argmax(self.state_frequencies)\n",
    "        else:\n",
    "            predicted_state = self.predict_state()\n",
    "        q_values = self.Q[predicted_state]\n",
    "        probabilities = self.softmax(q_values)\n",
    "        action = np.random.choice(len(q_values), p=probabilities)\n",
    "        return action, predicted_state\n",
    "    \n",
    "    def update_Q(self, state, action, reward):\n",
    "        self.Q[state][action] = self.lr * reward + (1 - self.lr) * self.Q[state][action]\n",
    "\n",
    "\n",
    "def discretize_total_contribution(total_contribution, num_states, N, E):\n",
    "    max_total = N * E\n",
    "    state = int(round((total_contribution / max_total) * (num_states - 1)))\n",
    "    state = min(state, num_states - 1)\n",
    "    return state\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, N, E, MR, num_rounds, temperature=10, lr=0.5, num_states=11, num_actions=11, incentive=0):\n",
    "        self.N = N                  \n",
    "        self.E = E                 \n",
    "        self.MR = MR            \n",
    "        self.num_rounds = num_rounds\n",
    "        self.initial_temperature = temperature\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.incentive = incentive \n",
    "        self.agents = [Agent(num_states=self.num_states, num_actions=self.num_actions, temperature=self.initial_temperature, lr=self.lr, endowment=self.E, MPCR=self.MR/self.N, incentive=self.incentive) for _ in range(N)]\n",
    "        self.action_values = np.linspace(0, self.E, num=self.num_actions)\n",
    "    \n",
    "    def run_simulation(self, plot=True, num_frames=20):\n",
    "        total_contributions_over_time = []\n",
    "        prev_total_contribution = 0\n",
    "        prev_state = discretize_total_contribution(prev_total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "        for round_num in range(self.num_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action()\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * (1+self.incentive) * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions] # proportional incentive\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        final_contributions = []\n",
    "        final_rounds = 1\n",
    "        for agent in self.agents:\n",
    "            agent.temperature = 0 \n",
    "\n",
    "        for round_num in range(final_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action(use_most_likely_state=True)\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "            final_contributions.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * (1+self.incentive) * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions] # proportional incentive\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        average_final_contribution = np.mean(final_contributions)\n",
    "        self.result = average_final_contribution\n",
    "        self.record = total_contributions_over_time\n",
    "        \n",
    "        #plotting part\n",
    "        \n",
    "        if plot:\n",
    "            # Set up the figure and axis with a dark background\n",
    "            plt.style.use('dark_background')  # Use a built-in dark style\n",
    "            plt.rcParams['font.family'] = 'Arial'\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            max_x = self.num_rounds + final_rounds\n",
    "            max_y = self.N * self.E\n",
    "\n",
    "            # Customize the plot appearance\n",
    "            ax.set_facecolor('#121212')  # Set axis background to a dark gray\n",
    "            fig.patch.set_facecolor('#121212')  # Set figure background to match\n",
    "\n",
    "            # Customize grid lines\n",
    "            ax.grid(True, color='#2A2A2A')  # Darker grid lines\n",
    "\n",
    "            # Initialize the line plot with custom colors\n",
    "            line, = ax.plot([], [], color='#00FF00', marker='o', markerfacecolor='#0335ff', markeredgecolor='#0335ff')\n",
    "            ax.set_xlim(1, max_x)\n",
    "            ax.set_ylim(0, max_y)\n",
    "            ax.set_title('Average Total Donation Over Time', color='white', fontsize=16)\n",
    "            ax.set_xlabel('Round', color='white', fontsize=14)\n",
    "            ax.set_ylabel('Average Total Donation', color='white', fontsize=14)\n",
    "\n",
    "            # Customize tick parameters\n",
    "            ax.tick_params(axis='x', colors='white')\n",
    "            ax.tick_params(axis='y', colors='white')\n",
    "\n",
    "            # Precompute start and end indices for each frame\n",
    "            total_rounds = len(total_contributions_over_time)\n",
    "            frame_sizes = total_rounds // num_frames\n",
    "            start_indices = [i * frame_sizes for i in range(num_frames)]\n",
    "            end_indices = [(i + 1) * frame_sizes for i in range(num_frames)]\n",
    "            end_indices[-1] = total_rounds  # Ensure the last frame includes any remaining rounds\n",
    "\n",
    "            # Precompute averages and x positions for each frame\n",
    "            frame_avgs = []\n",
    "            x_positions = []\n",
    "            for i in range(num_frames):\n",
    "                s_idx = start_indices[i]\n",
    "                e_idx = end_indices[i]\n",
    "                avg = np.mean(total_contributions_over_time[s_idx:e_idx])\n",
    "                frame_avgs.append(avg)\n",
    "                # Choose the x position as the end round number\n",
    "                x_pos = e_idx  # Use the end index as x-position\n",
    "                x_positions.append(x_pos)\n",
    "\n",
    "            # Function to update the plot for each frame\n",
    "            def update(frame):\n",
    "                x = x_positions[:frame + 1]\n",
    "                y = frame_avgs[:frame + 1]\n",
    "                line.set_data(x, y)\n",
    "                return line,\n",
    "\n",
    "            # Create the animation\n",
    "            ani = animation.FuncAnimation(fig, update, frames=range(num_frames), interval=100, blit=True)\n",
    "            ani.save('sample.gif', writer='pillow')\n",
    "            plt.show()\n",
    "            print(f\"Total rational contribution after learning: {average_final_contribution}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa9047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters to fix\n",
    "N = 10\n",
    "E = 100 \n",
    "MR = 5\n",
    "num_rounds = 1000\n",
    "temperature = 10\n",
    "lr = 1\n",
    "\n",
    "#parameter to be adjustable by website users ... range is 0~1\n",
    "incentive = 0.1\n",
    "\n",
    "#execution: should give you a gif for graph animation. \n",
    "# for the plotting part and graph animation gif, please modify to make it aligned with the website style, and to make it look more attractive.\n",
    "env = Environment(N=N, E=E, MR=MR, num_rounds=num_rounds, temperature=temperature, lr=lr, incentive=incentive)\n",
    "env.run_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f85566f",
   "metadata": {},
   "source": [
    "# For Transparency Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5881221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, temperature, lr, endowment, MPCR, transparency):\n",
    "        actions = np.linspace(0, endowment, num=num_actions)\n",
    "        self.Q = np.array([[np.random.normal((endowment - action + (MPCR) * action), (endowment - action + (MPCR) * action) * temperature/100) for action in actions] for _ in range(num_states)])\n",
    "        self.temperature = temperature  \n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.state_counts = np.full(num_states, temperature)\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def update_state_counts(self, observed_state):\n",
    "        self.state_counts[observed_state] += 1\n",
    "        self.state_frequencies = self.state_counts / np.sum(self.state_counts)\n",
    "    \n",
    "    def predict_state(self):\n",
    "        predicted_state = np.random.choice(self.num_states, p=self.state_frequencies)\n",
    "        return predicted_state\n",
    "    \n",
    "    def softmax(self, q_values):\n",
    "        if self.temperature > 0:\n",
    "            max_q = np.max(q_values)\n",
    "            exp_q = np.exp((q_values - max_q) / self.temperature)\n",
    "            probabilities = exp_q / np.sum(exp_q)\n",
    "        else:\n",
    "            probabilities = np.zeros_like(q_values)\n",
    "            probabilities[np.argmax(q_values)] = 1.0\n",
    "        return probabilities\n",
    "\n",
    "    def choose_action(self, use_most_likely_state=False):\n",
    "        if use_most_likely_state:\n",
    "            predicted_state = np.argmax(self.state_frequencies)\n",
    "        else:\n",
    "            predicted_state = self.predict_state()\n",
    "        q_values = self.Q[predicted_state]\n",
    "        probabilities = self.softmax(q_values)\n",
    "        action = np.random.choice(len(q_values), p=probabilities)\n",
    "        return action, predicted_state\n",
    "    \n",
    "    def update_Q(self, state, action, reward):\n",
    "        self.Q[state][action] = self.lr * reward + (1 - self.lr) * self.Q[state][action]\n",
    "\n",
    "\n",
    "def discretize_total_contribution(total_contribution, num_states, N, E):\n",
    "    max_total = N * E\n",
    "    state = int(round((total_contribution / max_total) * (num_states - 1)))\n",
    "    state = min(state, num_states - 1)\n",
    "    return state\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, N, E, MR, num_rounds, temperature=10, lr=0.5, num_states=11, num_actions=11, transparency=1):\n",
    "        self.N = N                  \n",
    "        self.E = E                 \n",
    "        self.MR = MR            \n",
    "        self.num_rounds = num_rounds\n",
    "        self.initial_temperature = temperature\n",
    "        self.lr = lr\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.transparency = transparency\n",
    "        self.agents = [Agent(num_states=self.num_states, num_actions=self.num_actions, temperature=self.initial_temperature, lr=self.lr, endowment=self.E, MPCR=self.MR/self.N, transparency=self.transparency) for _ in range(N)]\n",
    "        self.action_values = np.linspace(0, self.E, num=self.num_actions)\n",
    "    \n",
    "    def run_simulation(self, plot=True, num_frames=20):\n",
    "        total_contributions_over_time = []\n",
    "        prev_total_contribution = 0\n",
    "        prev_state = discretize_total_contribution(prev_total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "        for round_num in range(self.num_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action()\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * self.transparency * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions]\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        final_contributions = []\n",
    "        final_rounds = 1\n",
    "        for agent in self.agents:\n",
    "            agent.temperature = 0 \n",
    "\n",
    "        for round_num in range(final_rounds):\n",
    "            contributions = []\n",
    "            actions = []\n",
    "            predicted_states = []\n",
    "\n",
    "            for agent in self.agents:\n",
    "                action, predicted_state = agent.choose_action(use_most_likely_state=True)\n",
    "                contribution = self.action_values[action]\n",
    "                contributions.append(contribution)\n",
    "                actions.append(action)\n",
    "                predicted_states.append(predicted_state)\n",
    "\n",
    "            total_contribution = sum(contributions)\n",
    "            total_contributions_over_time.append(total_contribution)\n",
    "            final_contributions.append(total_contribution)\n",
    "\n",
    "            payoff_per_agent = (self.MR * self.transparency * total_contribution) / self.N\n",
    "            payoffs = [(self.E - contrib + payoff_per_agent) for contrib in contributions]\n",
    "            \n",
    "            prev_state = discretize_total_contribution(total_contribution, self.num_states, self.N, self.E)\n",
    "\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                reward = payoffs[i]\n",
    "                agent.update_Q(predicted_states[i], actions[i], reward)\n",
    "                agent.update_state_counts(prev_state)\n",
    "\n",
    "        average_final_contribution = np.mean(final_contributions)\n",
    "        self.result = average_final_contribution\n",
    "        self.record = total_contributions_over_time\n",
    "        \n",
    "        if plot:\n",
    "            # Set up the figure and axis with a dark background\n",
    "            plt.style.use('dark_background')  # Use a built-in dark style\n",
    "            plt.rcParams['font.family'] = 'Arial'\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            max_x = self.num_rounds + final_rounds\n",
    "            max_y = self.N * self.E\n",
    "\n",
    "            # Customize the plot appearance\n",
    "            ax.set_facecolor('#121212')  # Set axis background to a dark gray\n",
    "            fig.patch.set_facecolor('#121212')  # Set figure background to match\n",
    "\n",
    "            # Customize grid lines\n",
    "            ax.grid(True, color='#2A2A2A')  # Darker grid lines\n",
    "\n",
    "            # Initialize the line plot with custom colors\n",
    "            line, = ax.plot([], [], color='#00FF00', marker='o', markerfacecolor='#0335ff', markeredgecolor='#0335ff')\n",
    "            ax.set_xlim(1, max_x)\n",
    "            ax.set_ylim(0, max_y)\n",
    "            ax.set_title('Average Total Donation Over Time', color='white', fontsize=16)\n",
    "            ax.set_xlabel('Round', color='white', fontsize=14)\n",
    "            ax.set_ylabel('Average Total Donation', color='white', fontsize=14)\n",
    "\n",
    "            # Customize tick parameters\n",
    "            ax.tick_params(axis='x', colors='white')\n",
    "            ax.tick_params(axis='y', colors='white')\n",
    "\n",
    "            # Precompute start and end indices for each frame\n",
    "            total_rounds = len(total_contributions_over_time)\n",
    "            frame_sizes = total_rounds // num_frames\n",
    "            start_indices = [i * frame_sizes for i in range(num_frames)]\n",
    "            end_indices = [(i + 1) * frame_sizes for i in range(num_frames)]\n",
    "            end_indices[-1] = total_rounds  # Ensure the last frame includes any remaining rounds\n",
    "\n",
    "            # Precompute averages and x positions for each frame\n",
    "            frame_avgs = []\n",
    "            x_positions = []\n",
    "            for i in range(num_frames):\n",
    "                s_idx = start_indices[i]\n",
    "                e_idx = end_indices[i]\n",
    "                avg = np.mean(total_contributions_over_time[s_idx:e_idx])\n",
    "                frame_avgs.append(avg)\n",
    "                # Choose the x position as the end round number\n",
    "                x_pos = e_idx  # Use the end index as x-position\n",
    "                x_positions.append(x_pos)\n",
    "\n",
    "            # Function to update the plot for each frame\n",
    "            def update(frame):\n",
    "                x = x_positions[:frame + 1]\n",
    "                y = frame_avgs[:frame + 1]\n",
    "                line.set_data(x, y)\n",
    "                return line,\n",
    "\n",
    "            # Create the animation\n",
    "            ani = animation.FuncAnimation(fig, update, frames=range(num_frames), interval=100, blit=True)\n",
    "            ani.save('sample.gif', writer='pillow')\n",
    "            plt.show()\n",
    "            print(f\"Total rational contribution after learning: {average_final_contribution}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters to fix (notice its different from free rider page)\n",
    "N = 10          \n",
    "E = 100 \n",
    "MR = 10\n",
    "num_rounds = 1000\n",
    "temperature = 10\n",
    "lr = 1\n",
    "\n",
    "#parameter adjustable by website user: range is 0~1\n",
    "transparency = 0.3\n",
    "\n",
    "#execution: should give you a gif for graph animation. \n",
    "# for the plotting part and graph animation gif, please modify to make it aligned with the website style, and to make it look more attractive.\n",
    "env = Environment(N=N, E=E, MR=MR, num_rounds=num_rounds, temperature=temperature, lr=lr, transparency=transparency)\n",
    "env.run_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
